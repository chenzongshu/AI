
机器学习就是把无序的数据转换成有用的信息。

成功的机器学习有四个要素：

- 数据、
- 转换数据的模型、
- 衡量模型好坏的损失函数
- 一个调整模型权重来最小化损失函数的算法

# 监督学习

这类算法必须知道预测什么，即目标变量的分类信息

**分类和回归属于监督学习**

# 无监督学习

此时数据没有类别信息，也不会给定目标值。

在无监督学习中，将数据集合分成由类似的对象组成的多个类的过程被称为**聚类**；
将寻找描述数据统计值的过程称之为**密度估计**

# 选择合适的算法

首先考虑使用机器学习算法的目的。如果想要预测目标变量的值，则可以选择监督学习算法，
否则可以选择无监督学习算法。

确定选择监督学习算法之后，需要进一步确定目标变量类型，如果目标变量是离散型，如是/否、1/2/3、― 冗或者红/黄/黑等，则可以选择分类器算法；如果目标变量是连续型的数值，如0.0~ 100.00、-999~999或者+00~-00等，则需要选择回归算法

其次需要考虑的是数据问题。我们应该充分了解数据，对实际数据了解得越充分，越容易创
建符合实际需求的应用程序。主要应该了解数据的以下特性：特征值是离散型变量还是连续型变
量，特征值中是否存在缺失的值，何种原因造成缺失值，数据中是否存在异常值，某个特征发生
的频率如何（是否罕见得如同海底捞针），等等。充分了解上面提到的这些数据特性可以缩短选
择机器学习算法的时间


# K-近邻算法

简单地说，谷近邻算法采用测量不同特征值之间的距离方法进行分

>优点：精度高、对异常值不敏感、无数据输入假定。
缺点：计算复杂度高、空间复杂度高。
适用数据范围：数值型和标称型。

## 工作原理

存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。

输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。

一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处,通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

## 总结

k-近邻算法是分类数据最简单最有效的算法

k-近邻算法是基于实例的学习，使用算法时我们必须有接近实际数据的训练样本数据。k-近邻算法必须保存全部数据集，如果训练数据集的很大，必须使用大量的存储空间。此外,由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。

k-近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均
实例样本和典型实例样本具有什么特征。

# 决策树

> 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特
征数据。
缺点：可能会产生过度匹配问题。
适用数据类型：数值型和标称型

## 原理

在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。

为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前已经正确地划分数据分类，无需进一步对数据集进行分割。

如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。如何划分数据子集的算法和划分原始数据集的方法相同，直到所有具有相同类型的数据均在一个数据子集内

## 信息增益

划分数据集的大原则是：将无序的数据变得更加有序

### 熵

熵定义为信息的期望值

# 朴素贝叶斯

基于概率论

> 优点：在数据较少的情况下仍然有效，可以处理多类别问题。
缺点：对于输入数据的准备方式较为敏感。
适用数据类型：标称型数据。

朴素贝叶斯是贝叶斯决策理论的一部分,什么是贝叶斯决策理论?

假设现在有两类数据,用p1(x,y)表示数据点(x,y)属于类别1的概率，用p2(x,y)表示数据点(x,y)属于类别2的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：

- 如果p1(x,y) > p2(x,y), 那么类别为1。
- 如果p2(x,y) > pl(x,y), 那么类别为2。

也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，**即选择具有最高概率的决策。**

# Logistic回归

什么是回归?

假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归,表示要找到最佳拟合参数集.

利用Logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类

>优点：计算代价不高，易于理解和实现。
缺点：容易欠拟合，分类精度可能不高。
适用数据类型：数值型和标称型数据。

为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和代入Sigmoid函数中，进而得到一个范围在0〜1之间的数值。任何大于0.5的数据被分人1类，小于0.5即被归人0类。所以，Logistic回归回归也可以被看成是一种概率估计。

# 支持向量机

> 优点：泛化错误率低，计算开销不大，结果易解释。
缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。
适用数据类型：数值型和标称型数据。
